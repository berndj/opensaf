#
#      -*- OpenSAF  -*-
#
# (C) Copyright 2008-2016 The OpenSAF Foundation
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
# or FITNESS FOR A PARTICULAR PURPOSE. This file and program are licensed
# under the GNU Lesser General Public License Version 2.1, February 1999.
# The complete license can be accessed from the following location:
# http://opensource.org/licenses/lgpl-license.php
# See the Copying file included with the OpenSAF distribution for full
# licensing terms.
#
# Author(s): Ericsson AB
#
#

WHAT IS THIS?

This directory (and subdirectories) contains a User Mode Linux - UML simulated
cluster environment for opensaf. Default a five node cluster will be started,
two controller nodes and three payload nodes.

The cluster environment is built using:
- Linux kernel
- Busybox
- opensaf rpms
- a few host libraries

All packages are downloaded from the net. Make sure to have proxy settings
(http_proxy variable) correct if needed. Downloaded packages are stored in the
'archive' subdirectories. Downloading the Linux kernel can take some time,
select a HTTP server closer to you if that is an issue.

For minimising the memory requirements for each UML instance, the major part of
its root file system consists of links to files on the host file system. This
concept is called a shadow root, see below.

See the README file in the uml subdirectory for more information about UML.


DIRECTORY STRUCTURE

$OSAF_UML_EXTRA
          If this environment variable is set, it points to a directory
          containing compressed tar archives to be extracted into the guest file
          system. This is useful if you want to customise your UML guests by
          adding extra programs, or replace programs or libraries with your own
          versions that may be different from the ones installed on the host
          system.

archive   This directory contains a number of configuration files for the
          cluster. The 'scripts' subdirectory contains the init scripts that are
          executed during UML startup. The scripts are copied from this
          directory to the shadow root by the build system. Do not edit.

bin       This directory contains shell scripts used during the installation and
          to start the cluster.  Users: Do not edit.  Maintainers: The
          install.sh script is sensitive to changes in the opensaf rpm spec
          files. This script is written this way so that a non root user can
          build. Otherwise 'rpm -i --root=..' could have been used.

etc       This directory contains a script that this executed very early in the
          UML startup. It is required by the fairly generic UML scripts to be
          located here. Do not edit.

$OSAF_UML_BUILD
          This directory contains the UML kernel build, the busybox build and
          UML utilities.


HOW TO BUILD

When the UML root file system is generated it will use 'make install' with
DESTDIR set to the UML root file system. Therefore, you must first configure and
build OpenSAF before you can build the UML environment. If you have enabled TIPC
using the --enable-tipc option to the ./configure script when configuring
OpenSAF, then OpenSAF will be configured to use TIPC in the generated UML
environment. Otherwise TCP will be used. If you have built OpenSAF with the
-DRUNASROOT option, then OpenSAF will be configured to run as the root user in
the generated UML environment. Otherwise OpenSAF will be configured to run as
user 'opensaf' and group 'opensaf'.

- Once you have configured and built OpenSAF, go to the UML cluster simulator
  directory (the directory containing this README file) and execute
  './build_uml' to build the UML environment.

Building means downloading, configuring & building packages from the internet,
creating shadow a root file system for OpenSAF nodes.  When 'build_uml' is
executed a second time, only the shadow root file systems are recreated.

Files created and generated are removed by 'clean_uml'. Files downloaded from
the internet are not deleted. They have to be manually deleted from uml/archive
to start from scratch.

Building adds the following directories under $OSAF_UML_BUILD:

rootfs     This directory contains a shadow root directory for controller and
           payload nodes. Do not edit.


HOW TO RUN

To start the cluster, either from XML or persistent store

   ./opensaf start [number of nodes]

To stop the cluster:

   ./opensaf stop

To clean persistent store:

   ./opensaf clean

When the persistent store is cleaned, the next start will be from XML.

Default memory size for each blade is 128M. You can change this using the
environment variable OSAF_UML_MEMSIZE, e.g.:

    export OSAF_UML_MEMSIZE=256M

By default "./opensaf start" will not spawn more than five terminals, even if
you start more nodes. You can still log in to the other nodes using the "telnet"
command from one of the terminals. To change the maximum number of terminals
started by "./opensaf start", set the OSAF_UML_MAX_TERMS environment variable,
e.g.:

    export OSAF_UML_MAX_TERMS=10

You can select to randomize a part of the MAC address assigned to a node each
time the node is started by setting the environment variable
OSAF_UML_DYNAMIC_MAC to 1:

    export OSAF_UML_DYNAMIC_MAC=1

It is possible to choose IP address range, and select IPv4 or IPv6 addresses,
using the IPADDRBASE environment variable. For example, to use IPv6 address you
can use the following command:

    export IPADDRBASE=fe80::

Running the cluster adds the following directories:

repl-opensaf This directory contains a simulated shared replicated disk
             partition. It is used to store opensaf configurations and log
             files.

             Soft links are created from directories in the shadow root file
             system into this directory.

rootfs/var/<node name> This directory is the node unique '/var' directory. It
                       contains e.g. the system log from each node, stdouts for
                       the components, core dumps and so on.


STARTUP SEQUENCE

1. A number of uml/bin/linux instances are started, each in its own xterm. Each
   UML instance is given a unique command line e.g. hostname which is the same
   as the node name in the XML configuration.

2. /sbin/init is executed and reads /etc/inittab, executes the 'sysinit' entry
   (/etc/init.d/rcS)

3. /etc/init.d/rcS executes /hostfs/etc/init.d/umlprep (etc/init.d/umlprep)

4. /etc/init.d/rcS unpacks the shadow root cpio archive

5. /etc/init.d/rcS executes /etc/init.d/*.rc (archive/scripts/*) in numerical
   order.

6. controller
   /etc/init.d/40opensaf.rc does the following:
   - setups the environment for opensaf
   - executes '/etc/init.d/opensafd start'

6. payload
   /etc/init.d/40opensaf.rc does the following:
   - setups the environment for opensaf
   - executes '/etc/init.d/opensafd start'


NETWORKING

The cluster environment uses the UML utility 'uml_switch' to create an isolated
network for the UML instances. 'uml_switch' is a process that implements a
virtual switch using a UNIX domain socket.

It is not possible to communicate between the host and an UML instance with this
network setup. If host network access is required, please read UML
documentation.


SOFTWARE REQUIREMENTS

UML should run on any Linux 3.x or 4.x host kernel with GNU C Library version
2.14 or later. 32-bit and 64-bit are verified.

The following Debian/Ubuntu packages are known to work. Also make sure that you
have installed the corresponding development packages for these libraries.

- bash          4.3
- libc6         2.23
- libgcc1       6.0.1
- libmnl0       1.0.3
- libncurses5   6.0
- libreadline6  6.3
- libstdc++6    5.3.1

OPTIONAL DEVELOPMENT TOOLS

Before running ./build_uml, you can optionally install any of the following
development tools on your host system:

gdb
lsof
ltrace
python
python3
strace
tcpdump
valgrind

If these tools are found on the build host, they will automatically be installed
in the generated UML environment.
